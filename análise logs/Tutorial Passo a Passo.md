---
state: "[[AL]]"
---

### Tutorial Passo a Passo: Implementa√ß√£o R√°pida de um Ambiente ELK/OpenSearch para Logs de Conex√£o

Este guia assume que voc√™ tem servidores Linux para a aplica√ß√£o, o MySQL e para o ELK/OpenSearch.

---

### Passo 1: Prepare Sua Aplica√ß√£o (Onde a M√°gica Come√ßa)

Esta √© a fase mais cr√≠tica, pois a qualidade dos seus logs depende dela.

1. **Configure o Logging Estruturado (JSON):**
    
    - **A√ß√£o:** Edite a configura√ß√£o da sua biblioteca de logging (ex: `logback.xml` para Java, `appsettings.json` para .NET, ou configura√ß√£o do seu logger para Python/Node.js) para que ela produza logs em **formato JSON**.
        
    - **Por qu√™:** Facilita imensamente o parsing autom√°tico e a estrutura dos dados no ELK/OpenSearch.
        
2. **Implemente o MDC para `request_id` e `connection_id`:**
    
    - **A√ß√£o:** No c√≥digo da sua aplica√ß√£o, use o **MDC (Mapped Diagnostic Context)** para adicionar um `request_id` √∫nico no in√≠cio de cada requisi√ß√£o (HTTP, mensagem de fila, etc.) e **remova-o no `finally`**. Se poss√≠vel, adicione tamb√©m o `connection_id` quando uma conex√£o de banco de dados for obtida do pool.
        
    - **Por qu√™:** Permite correlacionar todos os logs de uma √∫nica opera√ß√£o em todos os sistemas envolvidos. O `connection_id` rastreia o ciclo de vida da conex√£o.
        
3. **Adicione M√©tricas do Pool de Conex√µes:**
    
    - **A√ß√£o:** Configure seu pool de conex√µes (ex: HikariCP, c3p0) para logar periodicamente suas estat√≠sticas de uso (ativo, ocioso, esperando, total).
        
    - **Por qu√™:** Fornece uma vis√£o cont√≠nua da sa√∫de do seu pool, essencial para identificar esgotamento ou leaks.
        
4. **Logue Eventos de Conex√£o (Opcional, mas √ötil):**
    
    - **A√ß√£o:** Se seu framework n√£o fizer, adicione logs expl√≠citos de `connection_opened` e `connection_closed` com o `connection_id`.
        
    - **Por qu√™:** Oferece visibilidade granular sobre o ciclo de vida de cada conex√£o.
        

---

### Passo 2: Configure o MySQL (A Vis√£o do Banco de Dados)

1. **Ative o Slow Query Log:**
    
    - **A√ß√£o:** Edite o arquivo de configura√ß√£o do MySQL (`my.cnf` ou `my.ini`).
        
        Ini, TOML
        
        ```
        # my.cnf
        [mysqld]
        slow_query_log = 1
        slow_query_log_file = /var/log/mysql/mysql-slow.log
        long_query_time = 1  # Loga queries que demoram mais de 1 segundo
        log_queries_not_using_indexes = 1 # Opcional: loga queries sem √≠ndice
        ```
        
    - **Por qu√™:** Identifica consultas demoradas que podem prender conex√µes.
        
2. **Monitore o Error Log:**
    
    - **A√ß√£o:** Garanta que o `log_error` esteja configurado para um local acess√≠vel.
        
        Ini, TOML
        
        ```
        # my.cnf
        [mysqld]
        log_error = /var/log/mysql/error.log
        ```
        
    - **Por qu√™:** Cont√©m mensagens cr√≠ticas como `Too many connections`, `Deadlock detected`, e erros de mem√≥ria.
        

---

### Passo 3: Implemente os Coletores de Logs (Filebeat)

Instale o Filebeat em **todos os servidores** que geram logs (aplica√ß√£o e MySQL).

1. **Instala√ß√£o do Filebeat:**
    
    - **A√ß√£o:** Siga a documenta√ß√£o oficial para instalar o Filebeat no seu sistema operacional (geralmente via pacotes `apt` ou `yum`).
        
2. **Configura√ß√£o do Filebeat (`filebeat.yml`):**
    
    - **A√ß√£o:** Configure os `inputs` para os logs da sua aplica√ß√£o e do MySQL.
        
        YAML
        
        ```
        # /etc/filebeat/filebeat.yml
        filebeat.inputs:
        - type: filestream
          enabled: true
          paths:
            - /var/log/minha_aplicacao/*.log # Log da aplica√ß√£o
          fields:
            service_name: "minha-api"
            environment: "production"
            log_type: "application" # Usado no Logstash para identificar
          json.keys_under_root: true # Se seus logs forem JSON
          json.overwrite_keys: true
          json.add_error_key: true
        
        - type: filestream
          enabled: true
          paths:
            - /var/log/mysql/error.log # MySQL Error Log
          fields:
            service_name: "mysql"
            environment: "production"
            log_type: "mysql_error"
          # MySQL logs s√£o geralmente texto, ent√£o n√£o use json.* aqui
        
        - type: filestream
          enabled: true
          paths:
            - /var/log/mysql/mysql-slow.log # MySQL Slow Query Log
          fields:
            service_name: "mysql"
            environment: "production"
            log_type: "mysql_slowlog"
          multiline.pattern: '^# User@Host:' # Crucial para logs de m√∫ltiplas linhas
          multiline.negate: true
          multiline.match: after
        
        output.logstash:
          hosts: ["<IP_DO_SERVIDOR_LOGSTASH>:5044"]
          # ssl.enable: true # Habilite em produ√ß√£o
          # ssl.certificate_authorities: ["/etc/filebeat/certs/ca.crt"] # CA do Logstash
        ```
        
    - **Por qu√™:** Garante que todos os logs relevantes sejam coletados de forma confi√°vel e com metadados b√°sicos. O `multiline` para o Slow Query Log √© vital para agrupar entradas.
        
3. **Inicie o Filebeat:**
    
    - **A√ß√£o:** `sudo systemctl enable filebeat && sudo systemctl start filebeat`
        

---

### Passo 4: Configure o Logstash (O Cora√ß√£o do Processamento)

1. **Instala√ß√£o do Logstash:**
    
    - **A√ß√£o:** Siga a documenta√ß√£o oficial para instalar o Logstash no seu servidor central.
        
2. **Crie o Arquivo de Configura√ß√£o (`logstash.conf`):**
    
    - **A√ß√£o:** Use o exemplo fornecido na resposta anterior, ajustando os caminhos, IPs e os padr√µes `grok` para corresponderem exatamente aos seus formatos de log.
        
    - **Principais pontos a verificar:**
        
        - **`input`:** Porta do Beats (5044).
            
        - **`filter`:**
            
            - **`json` filter:** Se seus logs de aplica√ß√£o forem JSON.
                
            - **`grok` filters:** Para logs de texto (aplica√ß√£o fallback, MySQL Error Log, MySQL Slow Query Log). **Teste seus padr√µes `grok` com um Grok Debugger online para garantir que funcionam.**
                
            - **`mutate` filters:** Para adicionar `issue_type`, `event_type` e promover campos para o n√≠vel raiz.
                
            - **`date` filter:** Para normalizar todos os timestamps para `@timestamp`.
                
        - **`output`:** Endere√ßo do Elasticsearch/OpenSearch e nome do √≠ndice (`logs-%{+YYYY.MM.dd}`). Configure SSL e credenciais para produ√ß√£o.
            
    - **Por qu√™:** √â aqui que os logs s√£o transformados em dados ricos e estruturados, prontos para an√°lise.
        
3. **Inicie o Logstash:**
    
    - **A√ß√£o:** `sudo systemctl enable logstash && sudo systemctl start logstash`
        
    - **Depura√ß√£o:** Use `sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/main.conf --config.test_and_exit` para testar sua configura√ß√£o antes de iniciar o servi√ßo.
        

---

### Passo 5: Configure o Elasticsearch / OpenSearch (O Armaz√©m Inteligente)

1. **Instala√ß√£o e Configura√ß√£o:**
    
    - **A√ß√£o:** Instale o Elasticsearch ou OpenSearch (e Kibana/OpenSearch Dashboards) em seu servidor dedicado. Siga a documenta√ß√£o oficial.
        
    - **Seguran√ßa:** **IMPRESCIND√çVEL em produ√ß√£o:** Configure SSL/TLS e autentica√ß√£o (usu√°rios e senhas).
        
    - **Melhor Pr√°tica:** Alocar recursos adequados (CPU, RAM, disco r√°pido) e configurar pol√≠ticas de **ILM (Index Lifecycle Management)** para Elasticsearch ou **ISM (Index State Management)** para OpenSearch.
        
        JSON
        
        ```
        # Exemplo de pol√≠tica ILM/ISM (configure via Kibana/Dashboards ou API)
        PUT _ilm/policy/my_log_policy
        {
          "policy": {
            "phases": {
              "hot": {
                "min_age": "0ms",
                "actions": {
                  "rollover": { "max_age": "1d", "max_docs": 10000000, "max_size": "50gb" }
                }
              },
              "warm": {
                "min_age": "7d",
                "actions": {
                  "set_priority": { "priority": 50 },
                  "forcemerge": { "max_num_segments": 1 }
                }
              },
              "cold": {
                "min_age": "30d",
                "actions": {
                  "set_priority": { "priority": 0 },
                  "freeze": {}
                }
              },
              "delete": {
                "min_age": "90d",
                "actions": {
                  "delete": {}
                }
              }
            }
          }
        }
        ```
        
    - **Template de √çndice:** Crie um template para seus √≠ndices `logs-*` para garantir que os campos num√©ricos sejam mapeados corretamente. Fa√ßa isso no Kibana/Dashboards em "Stack Management" -> "Index Management" -> "Index Templates".
        
    - **Por qu√™:** Garante que seus dados sejam armazenados de forma eficiente, pesquis√°vel e que o volume de dados seja gerenciado automaticamente, evitando problemas de disco cheio.
        

---

### Passo 6: Configure o Kibana / OpenSearch Dashboards (A Janela para Seus Dados)

1. **Acesse a Interface:**
    
    - **A√ß√£o:** Acesse o Kibana/OpenSearch Dashboards via navegador (`http://<IP_DO_SERVIDOR_KIBANA>:5601`).
        
2. **Crie Padr√µes de √çndice:**
    
    - **A√ß√£o:** Em "Stack Management" (ou "OpenSearch Dashboards Management") -> "Index Patterns", crie um padr√£o `logs-*` e selecione `@timestamp` como seu campo de tempo.
        
    - **Por qu√™:** Permite que o Kibana reconhe√ßa e visualize seus √≠ndices de log.
        
3. **Crie Dashboards Essenciais para Conex√µes:**
    
    - **A√ß√£o:** V√° para a se√ß√£o "Dashboard" e comece a criar visualiza√ß√µes:
        
        - **Visualiza√ß√£o de Gr√°fico de Linha:**
            
            - **M√©trica:** Contagem de documentos.
                
            - **Eixo X:** `@timestamp` (intervalo autom√°tico).
                
            - **Quebrar s√©rie por:** `issue_type` (tipo de erro).
                
            - **Por qu√™:** Visualize picos de `pool_exhausted`, `mysql_too_many_connections`, `application_out_of_memory` ao longo do tempo.
                
        - **Visualiza√ß√£o de Gr√°fico de Linha/√Årea:**
            
            - **M√©trica:** M√©dia de `pool_active`, `pool_idle`, `pool_waiting`.
                
            - **Eixo X:** `@timestamp`.
                
            - **Por qu√™:** Mostra a sa√∫de do pool de conex√µes da aplica√ß√£o.
                
        - **Tabela de Dados:**
            
            - **Colunas:** `query_text`, `query_duration_ms`, `db_user`, `client_ip`.
                
            - **Filtro:** `event_type: "mysql_slow_query_log"`.
                
            - **Ordena√ß√£o:** Por `query_duration_ms` (descendente).
                
            - **Por qu√™:** Identifica rapidamente as queries mais lentas.
                
    - **Por qu√™:** Fornecem uma vis√£o imediata da sa√∫de do sistema e alertam sobre anomalias.
        
4. **Explore os Dados (Aba Discover):**
    
    - **A√ß√£o:** Use a aba "Discover" para buscar, filtrar e inspecionar logs individuais.
        
        - **Filtre por `issue_type`:** Para ver apenas os logs de problemas (`issue_type: "pool_exhausted"`).
            
        - **Rastreie por `request_id`:** O **passo mais poderoso**. Digite `request_id: "seu-id-unico-aqui"` na barra de busca para ver todos os logs de uma requisi√ß√£o.
            
    - **Por qu√™:** Permite mergulhar nos detalhes para correlacionar eventos e encontrar a causa raiz.
        

---

Ufa! Com este roteiro, voc√™ ter√° uma base s√≥lida para monitorar e diagnosticar problemas de conex√£o de forma proativa. O investimento inicial vale a pena para a paz de esp√≠rito e a agilidade na resolu√ß√£o de incidentes.

Agora sim, pode respirar aliviado! üòâ